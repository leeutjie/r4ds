---
title: "R For Data Science Part I: Explore"
author: "Louis Becker"
output:
  html_document:
    toc: true
    toc_float: true
    theme: lumen
---

<br>

*R For Data Science Part I: Explore* is part of a series that serves as a condensed summary of the [R for Data Science](http://r4ds.had.co.nz/index.html) book as I work through it. The purpose is to use this series as a quick reference in future.

First, we install/load the relevant packages by installing the [tidyverse](https://www.tidyverse.org/) library. This "package" effectively contains all packages developed by *the* [Hadley Wickham](http://hadley.nz/) that relate to the [tidy data](https://www.jstatsoft.org/article/view/v059i10) way of thinking. This chapter focuses on specific libraries, but we install and load the whole `tidyverse` anyway:

```{r load, message=FALSE, warning=FALSE}

if (!require("tidyverse")) install.packages("tidyverse")
library("tidyverse")

```


# Data Visualisation

We start with some scatter plots of the *mpg* dataset, **Scatter 1**.

```{r scatter1}

ggplot(mpg) + 
  geom_point(aes(displ, hwy)) + 
  ggtitle("Scatter 1")

```


We could (and often should) be explicit about which parameters we are assigning variables to. Notice the *mapping*, *x* and *y* parameters that have been inserted into the code used to generate the previous graph:

```{r parameters, message=FALSE, warning=FALSE}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  ggtitle("Scatter 1")

```


The function `ggplot()` creates an empty graph off the underlying data set. The `geom_` functions then add a layers on top of this object. 

```{r scatter2}

# Practice
ggplot(mpg) + 
  geom_point(aes(y = hwy, x = cyl)) + 
  ggtitle("Scatter 2")

```


Here is another example:

```{r scatter3}

ggplot(mpg) + 
  geom_point(aes(y = class, x = drv)) + 
  ggtitle("Scatter 3")

```


**Scatter 3** is not a useful plot because the variables shown here cannot have a linear relationship. 

An epic feature of the `ggplot2` package is that it Can add a third variable to a two-dimensional plot by mapping this variable to an aesthetic, which is the visual property of the objects in my plot (size, shape, colour of points). This is described as the level. We can use the colour, size, shape and alpha (transparency) arguments in the `aesthetic` function to portray the third variable! This is known as scaling. In essence, we just map the aesthetic and ggplot does the rest:

```{r scatter4}

ggplot(mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) + 
  ggtitle("Scatter 4")

```


Possible aesthetics:

* alpha (transparency)
* colour
* fill (this aesthetic can also be used in a variable capacity to show different colours for different observations)
* group (use categorical variable here. It can replace color, and groups objects on the plot. See further down)
* shape
* size
* stroke (understand a little)

You can also map subject to certain constraints e.g. 

```{r}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = cyl < 6))

```

This will show which of the observations adhere to a particular condition or not. 


## Facets

One way to add additional variables is with aesthetics. Another way, which is particularly useful for categorical variables, is to split your plot into **facets** or subplots that each display one subset of the data.

```{r facet}

ggplot(mpg) +
  geom_point(aes(x = cyl, y = hwy, col = trans)) +
  facet_wrap(~ class, nrow = 3)

```

Note: "The first argument of facet_wrap() should be a formula, which you create with `~` followed by a variable name (here “formula” refers to the name of a data structure in R. It not a synonym for “equation”). The variable that you pass `to facet_wrap()` should be discrete."  

Facets can be done for two variables. To do this, add `facet_grid()` to the plot call. The first argument of this mapping is also a formula. 

```{r 2facet}

ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

```

To facet with just one variable, use a "." at the other end of the formula. 


## Geoms

"A geom is the geometrical object that a plot uses to represent data. People often describe plots by the type of geom that the plot uses." 

We can turn this... 
```{r point_geom}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

```


...into this...

```{r}

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))

# Loess regression is the default

```


...by changing the *geom*. Every *geom* takes a mapping argument, but not every `aes()` works with every *geom*. For example, with line types:

```{r}

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))

```


Many geoms use a single geometric object to display multiple rows of data. For these geoms, you can set the group aesthetic to a categorical variable to draw multiple objects (or use color, as previously mentioned).

We can plot multiple *geoms* for the same data and layer them over each other. This, however, causes repetition in the code. If, for example, the axes change, every geom's *x* and *y* specs would have to change. 

```{r multi_layers}

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

```


In the case of layering different geoms, we can specify parameters like the axes as global options in the *ggplot* chart object: 

```{r global}

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth()

 # Same data passed to different geoms. 

```


Epicly enough, we can still add features to each *geom* that will affect that layer only. For example, notice the colour in the following:

```{r layering}

ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()

```


I can also subset the data in a particular *geom* with `filter()` from the *dplyr* package. 

```{r}

ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point(aes(color = class)) +
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)

# se = TRUE/FALSE displays confidence interval around the geom_smooth line or not. 

```


## Statistical Transformations

Consider a bar chart based on the *diamonds* data set: 

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))
```


As you can see, this graph does not use a *y*-variable. This is because a bar chart creates new values to plot by design. It works with the number of observations within a range or bin and displays the count per bin. This is the same for histograms and frequency polygons. The algorithm that calculates these new values for the graph is called a `stat()`, short short for statistical transformation. 

You can learn which stat a geom uses by inspecting the default value for the stat argument. For example, `?geom_bar` shows that the default value for stat is “count”, which means that `geom_bar()` uses `stat_count()`. `stat_count()` is documented on the same page as `geom_bar()`, and if you scroll down you can find a section called “Computed variables”. That describes how it computes two new variables: *count* and *prop*.

Generally, *stats* and *geoms* are interchangable. We recreate the same plot with the following. 

```{r}

ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))

```

This works because every *geom* has a default *stat*; and every *stat* has a default *geom*. This means that you can typically use *geoms* without worrying about the underlying statistical transformation. There are three reasons you would use `stat()` explicitly:

1) Overiding the default stat in a particular ggplot because you want a different stat. In the example below we change the default stat from "count" to "identity". Identity uses the actual value assigned to a variable instead of its count. 

```{r res1}

# Create a small table with the tribble command.
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

# Now plot the demo table, but override the default stat in the bar chart geom. 
# The count geom is overriden to 
ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")

```

2) Override the default mapping of transformed variables e.g. display proportion on y-axis rather than count. 
```{r eh}

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1))

```

3) You might want to draw greater attention to the statistical transformation in your code.

```{r}

ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.ymin = min,
    fun.ymax = max,
    fun.y = median
  )

```


## Transformation Exercises
```{r}

ggplot(data = diamonds) + 
  geom_pointrange(mapping = aes(x = cut, y = depth, ymin = min(diamonds$depth), ymax = max(diamonds$depth))) #Close enough

# Automatically groups according to cut, or x variable. 
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = ..prop..))

# Needs to group relative to whole x variable. Can say group=1 or group = "x"
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..,  group = 1))

```


## Position  Adjustments
You can colour a bar chart using the `colour` or `fill` aesthetics. The fill `fill` argument is more useful, since you can map a variable to this aesthetic to display different colours. A stacked bar chart of sorts.


```{r fill}

# The "colour aesthetic" adds the border. 
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))

```


```{r}

# The "fill" aesthetic adds the interior colour. 
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))

```

The `fill` aesthetic can also be used as a variable to show different combinations. 
```{r}

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))

```

The stacking is performed automatically by the **position adjustment** specified by the `position` argument. If you don’t want a stacked bar chart, you can use one of three other options: "identity", "dodge" or "fill".

### position = identity
position = "identity" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it *overlaps* them vertically. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA.

```{r}
# Position = identity uses the raw value of the observation. Here, it places the bar at it's exact location
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 0.2, position = "identity")

# Here we specify the fill as NA. and empty the bars. 
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")

```

The identity position adjustment is more useful for 2d geoms, like points, where it is the default.

### position = fill
position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.

```{r}

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")

```


### position = dodge
`position = "dodge"` places overlapping objects directly *beside* one another. This makes it easier to compare individual values.

```{r}

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")

```

### Something interesting: position = jitter
Recall our first scatterplot.
```{r}
ggplot(mpg) + geom_point(aes(displ, hwy)) + ggtitle("Scatter 1")
```

This plot contains only 126 points even though there are 234 observations in the dataset. The values of hwy and displ are rounded so the points appear on a grid and many points overlap each other. This problem is known as __overplotting__. This arrangement makes it hard to see where the mass of the data is.

This gridding and overplooting can be avoided by setting position to "jitter". Position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise. Here is an expample:

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")

```

Adding randomness seems like a strange way to improve your plot, but while it makes your graph less accurate at small scales, it makes your graph more revealing at large scales. 

Because this is such a useful operation, ggplot2 comes with a shorthand for `geom_point(position = "jitter")`: `geom_jitter()`.

To learn more about a position adjustment, look up the help page associated with each adjustment: `?position_dodge`, `?position_fill`, `?position_identity`, `?position_jitter`, and `?position_stack`.


## Position Adjustment exercises
```{r}

# How to fix this 
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_point()

# Do this:
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_count()

ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_jitter()

# Default postion is dodge, putting data displays next to each other. 
ggplot(data = mpg, mapping = aes(x = trans, y = hwy)) + 
  geom_boxplot()

```


## Coordinate Systems
The default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point. There are a number of other coordinate systems that are occasionally helpful.

### Coord_flip()
`coord_flip()` switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.

```{r}

# Normal Plot
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()

# Add new coordinate system:
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()


```

### Coord_quickmap()
`coord_quickmap()` sets the aspect ratio correctly for maps. This is very important if you’re plotting spatial data with ggplot2.

```{r message=FALSE, warning=FALSE}
if (!require("maps")) install.packages("maps")
library(maps)

nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

# Quickmap sets the correct aspect ratio 
ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
```

### Coord_polar()
`coord_polar()` uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.

```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

# flipped bar chart
bar + coord_flip()

# Coxcomb chart
bar + coord_polar()
```

## Coordinate Systems Exercises
```{r}

ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() +
  geom_abline() + #adds line to graph by slope and intercept
  coord_fixed() # Default ratio is 1. 1 unit of x is the same as 1 unit of y

```

## Layered Grammar of Graphics
Read more on that [here](http://r4ds.had.co.nz/data-visualisation.html#the-layered-grammar-of-graphics)

To conclude, the chunk below shows the basic syntax structure of `ggplot` grammar:

```{r}

# ggplot(data = <DATA>) + 
#   <GEOM_FUNCTION>(
#      mapping = aes(<MAPPINGS>),
#      stat = <STAT>, 
#      position = <POSITION>
#   ) +
#   <COORDINATE_FUNCTION> +
#   <FACET_FUNCTION>
  
```



# Data Transformation

In this section we also use an additional dataset that can be installed in the form of packages:

```{r data, message=FALSE, warning=FALSE}

if (!require("nycflights13")) install.packages("nycflights13")
library("nycflights13")

```

The *dplyr* package works really well with a data object called a __tibble__. Essentially, a tibble is a nicely printable version of a [data frame](http://www.r-tutor.com/r-introduction/data-frame). Tibbles are data frames, but slightly tweaked to work better in the tidyverse. For now, you don’t need to worry about the differences; we’ll come back to tibbles in more detail in the Data Wrangle chapter.

When viewing a `tibble`, these are the common data types in the columns you'll come across:

* `int` - integers

* `dbl` - doubles, or real numbers

* `chr` - character vectors or strings

* `dttm` - date-times

* `lgl` - logicals/booleans -> TRUE or FALSE

* `fctr` - factor: which R uses to represent categorical variables with fixed possible values

* `date` - Dates


When working with a `tibble` or a `data.frame` in **dplyr**, the following data data _verbs_ are most commonly used for wrangling data:

* `filter()` - picking observations by their values

* `arrange()` - order your table by rows

* `select()` - pick variables by their names (by column)

* `mutate()` - create new variables with functions based on existing variables

* `summarise()` - collapse many variables down into a single summary

* `group_by()` - this verb is used in conjunction with the above. It changes the scope of each function from operating on the entire dataset to operating on it group-by-group.


## Filter()

* `filter()` can take the following comparison operators: `>`, `>=`, `<`, `<=`, `!=` (not equal), and `==` (tests if equal to).

* `filter()` can also accomodate logical operators:  `&` is “and”, `|` is “or”, `!` is “not” and `%in%` locates in a vector or concatenation. Sometimes you can simplify complicated subsetting by remembering De Morgan’s law: `!(x & y)` is the same as `!x | !y`, and `!(x | y)` is the same as `!x & !y`.

* `filter()` also handles NA's. You can filter NA's with the `is.na()` function. 

* `filter()` can also be used in conjunction with `between()` which is a shortcut for `x >=` left & `x <=` right, implemented efficiently in C++ for local values, and translated to the appropriate SQL syntax for remote tables.

* `filter()` only includes rows where the condition is `TRUE`; it excludes both `FALSE` and `NA` values. If you want to preserve missing values, ask for them explicitly, as in the example below:

```{r}

df <- tibble(x = c(1, NA, 3))
filter(df, x > 1)

filter(df, is.na(x) | x > 1)

```


## Arrange()

This orders the rows in a tibble. As more column names are provided, each additional column is used to break ties found in preceding columns. Use this verb in conjunction with `desc()` to sort in descending order. 


## Select()

`select()` allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. Here are a few examples:

```{r}

# Select columns by name
select(flights, year, month, day)

# Select all columns between year and day (inclusive)
select(flights, year:day)

# Select all columns except those from year to day (inclusive)
select(flights, -(year:day))

```

There are also a couple of helper functions that can be used within `select()`:

* `starts_with("abc")` - matches names that begin with “abc”.

* `ends_with("xyz")` -  matches names that end with “xyz”.

* `contains("ijk")` -  matches names that contain “ijk”.

* `matches("(.)\\1")` - selects variables that match a regular expression (more on this later). 

* `num_range("x", 1:3)` - matches `x1`, `x2` and `x3`.


## Mutate()

`mutate()` always adds new columns at the end of your dataset. Here is an example with a thinner dataset:

```{r}
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)

mutate(flights_sml,
  gain = dep_delay - arr_delay,
  speed = distance / air_time * 60
)
```

Note that you can refer to columns that you’ve just created:
```{r}
mutate(flights_sml,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

If you only want to keep the new variables, use `transmute()`:
```{r}
transmute(flights,
  gain = dep_delay - arr_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

There are many functions for creating new variables that you can use with `mutate()`. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful:

* Arithmetic operators: `+`, `-`, `*`, `/`, `^`. These are also useful with aggregate functions like `x / sum(x)` or `y - mean(y)`.

* Modular arithmetic: `%/%` (integer division) and `%%` (remainder), where `x == y * (x %/% y) + (x %% y)`. Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the flights dataset, you can compute `hour` and `minute` from `dep_time` with:
```{r}
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
```

* Logs: `log()`, `log2()`, `log10()`. These are useful transformations.

* Offsets: `lead()` and `lag()` allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. `x - lag(x)`) or find when values change (`x != lag(x)`). They are most useful in conjunction with `group_by()`.

* Cumulative and rolling aggregates: From base R, the following functions run cumulative sums, products, mins and maxes respectively: `cumsum()`, `cumprod()`, `cummin()` and `cummax`. *dplyr* provides `cummean()`. Finally, the *RcppRoll* package can provide rolling aggregates. 

* Logical comparisons: `<`, `<=`, `>`, `>=`, `!=`.

* Ranking: Many types of ranking functions exist, but `min_rank()` and `desc()` are good ones to start with. See also  `row_number()`, `dense_rank()`, `percent_rank()`, `cume_dist()`, `ntile()`. 

## Summarise()

This data verb collapses a data frame into a single row. 
```{r}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

`summarise()` is not terribly useful unless we pair it with `group_by()`. Adding `group_by()` changes the unit of analysis from the complete dataset to individual groups. Here is an example where flights are grouped by day, and the average departure time is calculated. 

```{r}
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```



At this point it would be useful to read the short sections on [combining operations with the pipe operator](http://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe) and [missing values](http://r4ds.had.co.nz/transform.html#missing-values-1), also in this chapter. 

The nice thing about `summarise()` is that you can add multiple summaries within the same function:

```{r}
flights %>% 
  group_by(dest) %>% 
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% 
  filter(count > 20, dest != "HNL")
```


Whenever you do any aggregation, it’s always a good idea to include either a count `(n())`, or a count of non-missing values `(sum(!is.na(x)))`. This  way you can check that you’re not drawing conclusions based on very small amounts of data. Here is a quick example of what such an implementation would look like:

```{r}

# Remove missing values
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

# Create counts and summaries of the data (average dealy by tail number vs the count of tail numbers per average delay)
delays <- 
  not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  )

# Plot the summarised data
ggplot(data = delays, mapping = aes(x = n, y = delay)) + 
  geom_point(alpha = 1/10)

```

Sometimes with these plots you want to subset the data to see if more of a pattern exists. The following snippet of code shows a nice way of integrating a chart into the piping functionality:

```{r}
delays %>% 
  filter(n > 25) %>% 
  ggplot(mapping = aes(x = n, y = delay)) + 
    geom_point(alpha = 1/10)
```

### Useful Summary Functions

Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions:

* **Measures of location**: Along with `mean()`, `median()` is also useful. It’s sometimes useful to combine aggregation with logical subsetting:
```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(
    avg_delay1 = mean(arr_delay),
    avg_delay2 = mean(arr_delay[arr_delay > 0]) # the average positive delay
  )
```

* **Measures of spread**: The root mean squared deviation, or standard deviation `sd()`, The interquartile range `IQR()` and median absolute deviation `mad(x)`

* **Measures of rank:**: `min(x)`, `quantile(x, 0.25)`, `max(x)`. Quantiles are a generalisation of the median. For example, `quantile(x, 0.25)` will find a value of x that is greater than 25% of the values, and less than the remaining 75%.

* *Measures of position*: `first(x)`, `nth(x, 2)`, `last(x)`. These work similarly to `x[1]`, `x[2]`, and `x[length(x)]` but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements). For example, we can find the first and last departure for each day:
```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(
    first_dep = first(dep_time), 
    last_dep = last(dep_time)
  )
```

+ These functions are complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:
```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(
    first_dep = first(dep_time), 
    last_dep = last(dep_time)
  )
```

* *Counts*: There is `n()`, which takes no arguments and returns the size of the current group. Use `sum(!is.na(x))` to count non-missing values and `n_distinct(x)` to count the number of unique values: 
```{r}
# Which destinations have the most carriers?
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(carriers = n_distinct(carrier)) %>% 
  arrange(desc(carriers))
```
  
  `dplyr`'s `count()` function is also extremely useful - it also does an automatic grouped count:
```{r}
not_cancelled %>% 
  count(dest)
```
  
  For this count function it is also possible to provide a weight variable. For example, you could use this to “count” (sum) the total number of miles a plane flew:
```{r}
not_cancelled %>% 
  count(tailnum, wt = distance)
```

* *Counts and proportions of logical values*: `sum(x > 10)`,` mean(y == 0)`. When used with numeric functions, `TRUE` is converted to 1 and `FALSE` to 0. This makes `sum()` and `mean()` very useful: `sum(x)` gives the number of `TRUE`s in `x`, and `mean(x)` gives the proportion.

```{r}
# How many flights left before 5am? (these usually indicate delayed
# flights from the previous day)
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(n_early = sum(dep_time < 500))
```

```{r}
# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(hour_perc = mean(arr_delay > 60))
```


### Grouping by Multiple Variables
When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

```{r}
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
```

```{r}
(per_month <- summarise(per_day, flights = sum(flights)))
```

```{r}
(per_year  <- summarise(per_month, flights = sum(flights)))
```

Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.


### Ungrouping

If you need to remove grouping, and return to operations on ungrouped data, use `ungroup()`.

```{r}
daily %>% 
  ungroup() %>%             # no longer grouped by date
  summarise(flights = n())  # all flights
```


### Grouped mutates (and filters)
Grouping is most useful in conjunction with `summarise()`, but you can also do convenient operations with `mutate()` and `filter()`:

* Find the worst members of each group:
```{r}
flights_sml %>% 
  group_by(year, month, day) %>%
  filter(rank(desc(arr_delay)) < 10)
```

* Find all groups bigger than a threshold:

```{r}
popular_dests <- flights %>% 
  group_by(dest) %>% 
  filter(n() > 365)
popular_dests
```

* Standardise to compute per group metrics:

```{r}
popular_dests %>% 
  filter(arr_delay > 0) %>% 
  mutate(prop_delay = arr_delay / sum(arr_delay)) %>% 
  select(year:day, dest, arr_delay, prop_delay)
```


Functions that work most naturally in grouped mutates and filters are known as window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: `vignette("window-functions")`.


# Answers to Exercises