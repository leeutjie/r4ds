---
output:
  html_document:
    toc: true
    toc_float: true
    theme: lumen
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

<br>

This section is part of a series that serves as a condensed summary of the [R for Data Science](http://r4ds.had.co.nz/index.html) book as I work through it. The purpose is to use this series as a quick reference in future.

First, we install/load the relevant packages by installing the [tidyverse](https://www.tidyverse.org/) library. This "package" effectively contains all packages developed by *the* [Hadley Wickham](http://hadley.nz/) that relate to the [tidy data](https://www.jstatsoft.org/article/view/v059i10) way of thinking. This chapter focuses on specific libraries, but we install and load the whole `tidyverse` anyway:

```{r load}
if (!require("tidyverse")) install.packages("tidyverse")
```


> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham

You can represent the same underlying data in multiple ways, but these representations are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse. If you’d like to learn more about the underlying theory of tidy data, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 

There are three interrelated rules which make a dataset tidy:
  
  1. Each variable must have its own column.
  
  2. Each observation must have its own row.
  
  3. Each value must have its own cell.


These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions:

  1. Put each dataset in a tibble.
  2. Put each variable in a column.

There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. Most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. dplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data. Here are a couple of small examples showing how you might work with `table1`.

# Pivoting

The first step to tidying data is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:

  1. One variable might be spread across multiple columns.

  2. One observation might be scattered across multiple rows.
  
To fix these problems, you’ll need the two most important functions in tidyr: `pivot_longer()` and `pivot_wider()`.

## Longer

A common problem is a dataset where some of the column names are not names of variables, but _values_ of a variable:

```{r}
table4a
```

To tidy a dataset like this, we need to *pivot* the offending columns into a new pair of variables:

```{r}
table4a %>% 
  
  pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "cases")
```

Note that “1999” and “2000” are non-syntactic names (because they don’t start with a letter) so we have to surround them in backticks. In the final result, the pivoted columns are dropped, and we get new `year` and `cases` columns. 


# Wider 

`pivot_wider()` is the opposite of `pivot_longer()`. You use it when an observation is scattered across multiple rows: 
 
```{r}
table2
```

To tidy this up, we first analyse the representation in similar way to pivot_longer(). This time, however, we only need two parameters:

  1. The column to take variable names from. Here, it’s `type`.

  2. The column to take values from. Here it’s `count`.

```{r}
table2 %>%
  
    pivot_wider(names_from = type, values_from = count)
```


# Separating and Uniting

## Separate

Sometimes the situation arises where we have one column that contains two variables:

```{r}
table3
```

The `rate` column contains both `cases` and `population` variables, which need to be split into two columns:

```{r}
table3 %>% 
  
  separate(rate, into = c("cases", "population"))
```

By default, separate() will split values wherever it sees a non-alphanumeric character (i.e. a character that isn’t a number or letter). In the above example, `separate()` split by `/`. It is also possible to specify the separator:

```{r}
table3 %>%   
  
  separate(rate, into = c("cases", "population"), sep = "/")
```

By default, `separate()` splits character columns. We can ask `separate()` to try and convert to better types using `convert = TRUE`:

```{r}
table3 %>% 
  
  separate(rate, into = c("cases", "population"), convert = TRUE)
```

You can also pass a vector of integers to `sep`, which will interpret the integers as _positions_ to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings.

```{r}
table3 %>% 
  
  separate(year, into = c("century", "year"), sep = 2)
```


## Unite

`unite()` is the inverse of `separate()`: it combines multiple columns into a single column. Below is an example that reunites the split columns from a above:

```{r}
table5 %>% 
  
  unite(new, century, year)
```

In this case we also need to use the sep argument. The default will place an underscore (`_`) between the values from different columns.


# Missing Values

Changing the representation of a dataset brings up an important subtlety of missing values. Surprisingly, a value can be missing in one of two possible ways:

 + *Explicitly* - flagged with an `NA`.
 + *Implicitly* - simply not in the data.
 
The following dataset illustrates this:

```{r}
stocks <- tibble(
  
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
)
```

2015 quarter has an `NA`, but 2016 quarter 1 is simply not there. 

> An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence.

The way that a dataset is represented can make implicit values explicit. For example, we can make the implicit missing value explicit by putting years in the columns:

```{r}
stocks %>% 
   
  pivot_wider(names_from = year, values_from = return)
```

Because these explicit missing values may not be important in other representations of the data, you can set `values_drop_na = TRUE` in `pivot_longer() `to turn explicit missing values implicit:

```{r}
stocks %>% 
  
  pivot_wider(names_from = year, values_from = return) %>% 
  
  pivot_longer(
    
    cols = c(`2015`, `2016`), 
    
    names_to = "year", 
    
    values_to = "return", 
    
    values_drop_na = TRUE
  )
```

`complete()` takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit `NAs` where necessary:

```{r}
stocks %>% 
  
  complete(year, qtr)
```

`fill()` takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward):

```{r}
treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
)

treatment %>% 
  fill(person)
```


# Case Study

The `tidyr::who` dataset contains tuberculosis (TB) cases broken down by year, country, age, gender, and diagnosis method. This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values.

The best place to start is almost always to gather together the columns that are not variables: 

  + It looks like `country`, `iso2`, and `iso3` are three variables that redundantly specify the country.

  + `year` is clearly also a variable.

  + We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. `new_sp_m014`, `new_ep_m014`, `new_ep_f014`) these are likely to be values, not variables.

We need to gather together all the columns from new_sp_m014 to newrel_f65. There are a lot of missing values in the current representation, so for now we’ll use `na.rm` just so we can focus on the values that are present.

```{r}
who1 <- 
  
  who %>% 
  
  pivot_longer(
    
    cols = new_sp_m014:newrel_f65, 
    
    names_to = "key", 
    
    values_to = "cases", 
    
    values_drop_na = TRUE
    
  )

who1
```

We can get some hint of the structure of the values in the `new` key column by counting them:

```{r}
who1 %>% 
  
  count(key)
```

We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel

```{r}
who2 <- 
  
  who1 %>% 
  
  mutate(names_from = stringr::str_replace(key, "newrel", "new_rel"))

who2
```

We can separate the values in each code with two passes of `separate()`. The first pass will split the codes at each underscore.

```{r}
who3 <- who2 %>% 
  
  separate(key, c("new", "type", "sexage"), sep = "_")
```

Let's drop some unnecessary co0lumns:

```{r}
who4 <- who3 %>% 
  
  select(-new, -iso2, -iso3)
```


Next we’ll separate sexage into `sex` and `age` by splitting after the first character:

```{r}
who5 <- who4 %>% 
  
  separate(sexage, c("sex", "age"), sep = 1)

who5
```

In a normal piping structure, this whole process would look as follows:

```{r}
who %>%
  
  pivot_longer(
    
    cols = new_sp_m014:newrel_f65, 
    
    names_to = "key", 
    
    values_to = "cases", 
    
    values_drop_na = TRUE
    
  ) %>% 
  
  mutate(
    
    key = stringr::str_replace(key, "newrel", "new_rel")
    
  ) %>%
  
  separate(key, c("new", "var", "sexage")) %>% 
  
  select(-new, -iso2, -iso3) %>% 
  
  separate(sexage, c("sex", "age"), sep = 1)
```


